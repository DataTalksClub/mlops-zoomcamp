{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 - Model Tracking Using MLflow\n",
    "In this section we discuss how to use [MLflow](https://mlflow.org/docs/latest/index.html) to track model experiments, save experiment and model information / artifacts to the mlflow database, learn about the model registry and choose models for staging and production,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Different MLflow Components\n",
    ":class: tip\n",
    "1. Model Tracking - For logging parameters, code versions, metrics etc.\n",
    "2. MLflow models - A packagng format for easy deployment on systems such as docker, Spark, Databricks etc.\n",
    "3. Model registry - A centralised model store that helps with the governance of models\n",
    "4. Projects - Standard format for packaging reusable data science code \n",
    "5. Recipes -  Predefined template for building models\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tracking: \n",
    "MLflow can log pretty much anything you need or think would be important to know about the model build. The different objects that can be tracked are broken down into 4 different distinct areas:\n",
    "\n",
    "| Name              | Used for                                                          |\n",
    "| :---------------- | :----------------------------------------------------------------:|\n",
    "| Parameters        |  Contstant values such as configuration params, model params etc  |\n",
    "| Metrics           |  Values updated during the run e.g. RMSE, Gini, etc.              |\n",
    "| Artifacts         |  Files produced by the run (Model weights, pipeline etc)          |\n",
    "| Additional        |  Items like creators names, tags etc                              |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's Great But Where is All The Information Stored?\n",
    "MLflow runs can be recorded to\n",
    "1. Local files,\n",
    "2. To a SQLAlchemy-compatible database, or\n",
    "3. Remotely to a tracking server. \n",
    "\n",
    "By default, the MLflow Python API logs runs locally to files in an mlruns directory wherever you ran your program. You can then run mlflow ui to see the logged runs.\n",
    "For us, because we are running it locally with a sqlite database, artifacts are stored in the `mlruns directory` and entities are stored in the `mlruns.db`\n",
    "\n",
    "__To log runs remotely, set the MLFLOW_TRACKING_URI environment variable to a tracking serverâ€™s URI or call mlflow.set_tracking_uri().__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Experiments Not Showing Up In The UI\n",
    ":class: warning\n",
    "When running MLflow you might notice that the experiments do not show up in the UI even though they are present in the `mlruns` directory. Your issue is probably that you ran the command `mlflow ui` outside of the directory that contains the mlruns folder.\n",
    "\n",
    "to fix this, simply navigate to the directory that contains the `mlruns` folder and rerun the ui command. Any tests that you have run and that have not been picked up will show up then!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/sh-mlops-zoomcamp/mlops_jupyter_book\")\n",
    "from utils.utils import ROOT_DIR, render_itable, init_jb_table_style, _import_data\n",
    "from itables import init_notebook_mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "init_jb_table_style()\n",
    "init_notebook_mode(all_interactive=True, connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MLFLOW_TRACKING_URI=\"sqlite:///mlflow.db\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/ubuntu/sh-mlops-zoomcamp/mlruns/1', creation_time=1685052038922, experiment_id='1', last_update_time=1685052038922, lifecycle_stage='active', name='jb-experiment', tags={}>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the tracking uri database to use and also setting the experiment name\n",
    "# If the experiment exists then we log in to that experiment. If not, it creates a new experiment\n",
    "%env MLFLOW_TRACKING_URI  \"sqlite:///mlflow.db\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"jb-experiment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autologging\n",
    "\n",
    "MLflow has a fantastic auto-logging function that logs everything about the training and testing parameters in a single step. The `autolog` function.\n",
    "\n",
    "For sklearn (Probably what we all we be using most of the time) it logs the following information:\n",
    "1. Training score obtained by estimator.score\n",
    "2. Parameters obtained by estimator.get_params\n",
    "3. Class name\n",
    "4. Fitted Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/05 23:33:49 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2023/06/05 23:33:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/ubuntu/anaconda3/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: d52e57b67cfb43e48b4c3b30fc336dca\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog()\n",
    "with mlflow.start_run(run_name=\"autolog\") as run:\n",
    "    # Load the diabetes dataset.\n",
    "    db = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "    # Create and train models.\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to make predictions on the test dataset.\n",
    "    predictions = rf.predict(X_test)\n",
    "\n",
    "    signature = infer_signature(X_test, predictions)\n",
    "    mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "\n",
    "    print(\"Run ID: {}\".format(run.info.run_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 09e382cef62a4291a36c82e75401dc11\n"
     ]
    }
   ],
   "source": [
    "mlflow.autolog(disable=True)\n",
    "\n",
    "with mlflow.start_run(run_name=\"no_autolog\") as run:\n",
    "    # Load the diabetes dataset.\n",
    "    db = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "    # Create and train models.\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    mlflow.log_params({'model_params' : rf.get_params()})\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to make predictions on the test dataset.\n",
    "    predictions = rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mlflow.log_metric('mse', mse)\n",
    "\n",
    "    signature = infer_signature(X_test, predictions)\n",
    "    mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "\n",
    "    print(\"Run ID: {}\".format(run.info.run_id))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Registry\n",
    "When you are training and deploying a bunch of models in production, it becomes difficult to track what models are live, what models are challenging and which models are archived. \n",
    "MLflow has a really handy tool called the `Model Registry` which keeps track of all of this information. this really helps when it comes to deploying the model and dealing with the ML engineers to get a model into production.\n",
    "\n",
    "```{note}\n",
    "The model registry doesn't actually deploy any models into production. It is only a place that lists which models are production ready\n",
    "```\n",
    "\n",
    "registered models have a unique name, contain versions, associated transitional stages, model lineage, and other metadata."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Models in the Registry: \n",
    "MLflows model registry has 3 types of models included:\n",
    "\n",
    "| Name              | Used for                                                          |\n",
    "| :---------------- | :----------------------------------------------------------------:|\n",
    "| Staging           |  Models that have been training and are going through testing against the current production model |\n",
    "| Production        |  Models that have completed testing / review and have been deployed to applications                |\n",
    "| Archived          |  Models that were in production / staging but are no longer used by any application                |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Move Models to Staging:\n",
    "Models can be moved to the model registry in 2 ways:\n",
    "1. In the UI\n",
    "2. Through the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "client = MlflowClient(tracking_uri=\"sqlite:///mlflow.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Experiment: artifact_location='/home/ubuntu/sh-mlops-zoomcamp/mlruns/4', creation_time=1685826086193, experiment_id='4', last_update_time=1685826086193, lifecycle_stage='active', name='optuna_test', tags={}>,\n",
       " <Experiment: artifact_location='/home/ubuntu/sh-mlops-zoomcamp/mlruns/3', creation_time=1685394893089, experiment_id='3', last_update_time=1685394893089, lifecycle_stage='active', name='exp_from_api_test', tags={}>,\n",
       " <Experiment: artifact_location='/home/ubuntu/sh-mlops-zoomcamp/mlruns/1', creation_time=1685052038922, experiment_id='1', last_update_time=1685052038922, lifecycle_stage='active', name='jb-experiment', tags={}>,\n",
       " <Experiment: artifact_location='/home/ubuntu/sh-mlops-zoomcamp/mlruns/0', creation_time=1685040306314, experiment_id='0', last_update_time=1685040306314, lifecycle_stage='active', name='Default', tags={}>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.search_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.create_experiment(name = 'exp_from_api_test')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = client.search_runs(experiment_ids='1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Id: 09e382cef62a4291a36c82e75401dc11, MSE: 3000.5021268418263\n"
     ]
    }
   ],
   "source": [
    "#access the run-id\n",
    "print(f\"Run Id: {runs[0].info.run_id}, MSE: {runs[0].data.metrics['mse']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'reg_model_test' already exists. Creating a new version of this model...\n",
      "2023/06/05 23:33:57 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: reg_model_test, version 9\n",
      "Created version '9' of model 'reg_model_test'.\n"
     ]
    }
   ],
   "source": [
    "run_id = runs[0].info.run_id\n",
    "result = mlflow.register_model(\n",
    "    f\"runs:/{run_id}/model\", \"reg_model_test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1685927318810, current_stage='Archived', description=None, last_updated_timestamp=1686008038031, name='reg_model_test', run_id=None, run_link=None, source='093f15e5885449bcb67cce76bb2be6dc', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "How to transition a model to different stages. The different stages are:\n",
    "1. Staging\n",
    "2. Production\n",
    "3. Archived\n",
    "'''\n",
    "client.transition_model_version_stage(\n",
    "    name=\"reg_model_test\", version=1, stage=\"Archived\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1685927318810, current_stage='Production', description=None, last_updated_timestamp=1686008038107, name='reg_model_test', run_id=None, run_link=None, source='093f15e5885449bcb67cce76bb2be6dc', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.transition_model_version_stage(\n",
    "    name=\"reg_model_test\", version=1, stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "source": [
    "model_name = \"reg_model_test\"\n",
    "model_version = 7\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "model.predict(X_test)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Model Registry Commands Through The API:\n",
    "\n",
    "| Name      | Use                                       | Function Name                             |\n",
    "|-----------|-------------------------------------------|-------------------------------------------|\n",
    "|Adding / Updating Descriptions               |Adding or Updating a Model Description|MlflowClient().rename_registered_model|\n",
    "|Renaming Models |Renaming a Model|MlflowClient().update_model_version|\n",
    "|Serving a Model      |Serving a Model as a service on our Host|export MLFLOW_TRACKING_URI=http://localhost:5000 <br /> mlflow models serve -m \"models:/sk-learn-random-forest-reg-model/Production\"|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting Experiments\n",
    "When you delete an experiment in the MLflow UI it gets sent to the .trash folder but it is not fully deleted. In this state you cannot make a new experiment with the same name as the experiment you just deleted which can be annoying. \n",
    "\n",
    "The MLflow CLI has a command that can permenently delete these front-end experiment deletions. Running the following in the command line will delete these experiments:\n",
    "`mlflow gc --backend-store-uri <PATH>`\n",
    "\n",
    "Example:\n",
    "` mlflow gc --backend-store-uri sqlite:///mlflow.db`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
